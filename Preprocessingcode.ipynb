{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd77a1d4",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19634ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942854ce",
   "metadata": {},
   "source": [
    "### Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6599190a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e450ca4",
   "metadata": {},
   "source": [
    "### Loading Dataset into test and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3d45a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85136a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of the training images data\")\n",
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fe2795",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of the testing images data\")\n",
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c297ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = np.unique(train_labels)\n",
    "print('Total number of unique classes:') \n",
    "len(unique_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c2cd8b",
   "metadata": {},
   "source": [
    "- **Training Images Shape**: (50,000, 32, 32, 3)\n",
    "- **Testing Images Shape**: (10,000, 32, 32, 3)\n",
    "\n",
    "The CIFAR-10 dataset consists of 60,000 32x32 color images across 10 classes. The training data contains 50,000 images while the test data contains 10,000 images. Each image has 3 color channels (RGB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6cabe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog','horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aca3871",
   "metadata": {},
   "source": [
    "Mapped the numeric labels (0–9) to their corresponding class names according to the official CIFAR-10 dataset specification for better visualisation and result interpretation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ee5831",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10)) \n",
    "\n",
    "for i in range(9):\n",
    "    index = random.randint(0, len(train_images) - 1) \n",
    "    image = train_images[index]  \n",
    "    label = train_labels[index][0]  \n",
    "\n",
    "    plt.subplot(3, 3, i+1) \n",
    "    plt.imshow(image)  \n",
    "    plt.title(class_names[label])\n",
    "    plt.axis('off')  \n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0b4d57",
   "metadata": {},
   "source": [
    "Displaying random samples from the dataset to understand the image quality, variety and classes before applying preprocessing techniques.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0128fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "index = random.randint(0, len(train_images) - 1)\n",
    "original_image = train_images[index]\n",
    "original_label = train_labels[index][0]\n",
    "print(f\"Index of selected image : {index}\")\n",
    "print(f\"Class of selected image : {class_names[original_label]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37edbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(original_image)\n",
    "plt.title(f\"Original Image: {class_names[original_label]}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca5b957",
   "metadata": {},
   "source": [
    "Selected one image using a random seed to ensure reproducibility. We will use this image to demonstrate all five preprocessing techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97081a8b",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87229028",
   "metadata": {},
   "source": [
    "-Normalization: Scales pixel values from [0, 255] to [0, 1] range.\n",
    "\n",
    "-Purpose : Neural networks train better with smaller values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b810417c",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_image = original_image / 255.0\n",
    "print(\"Original pixel range:\", original_image.min(), \"to\", original_image.max())\n",
    "print(\"Normalized pixel range:\", normalized_image.min(), \"to\", normalized_image.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dd8451",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axes[0].imshow(original_image)\n",
    "axes[0].set_title('Before Normalization\\n(Values: 0-255)', fontsize=12)\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(normalized_image)\n",
    "axes[1].set_title('After Normalization\\n(Values: 0-1)', fontsize=12)\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b65946d",
   "metadata": {},
   "source": [
    "The normalization comparison shows that while the visual appearance remains the same the underlying pixel values are transformed from the [0, 255] range to [0, 1]. This preprocessing step is essential for neural network training as it ensures numerical stability, faster convergence and prevents gradient-related issues during backpropagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbc640f",
   "metadata": {},
   "source": [
    "### Random Rotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73162c4a",
   "metadata": {},
   "source": [
    "-Rnadom Rotation: Rotates images by random angles (±20 degrees).\n",
    "\n",
    "-Purpose: Real-world objects in CIFAR-10 (airplanes, cars, animals) appear at different angles. Rotation augmentation increases dataset diversity, helps the model recognize objects regardless of orientation and prevents overfitting to specific orientations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5c8fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tensor = tf.expand_dims(original_image, axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312e541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotation_layer = tf.keras.layers.RandomRotation(factor=0.055)\n",
    "rotated_tensor = rotation_layer(image_tensor, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24cf8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotated_image = rotated_tensor[0].numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbc0e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axes[0].imshow(original_image.astype('uint8'))\n",
    "axes[0].set_title('Before Rotation', fontsize=12)\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(rotated_image.astype('uint8'))\n",
    "axes[1].set_title('After Random Rotation (±20°)', fontsize=12)\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a4b821",
   "metadata": {},
   "source": [
    "The rotated image shows the object at a different angle while preserving all visual features. This augmentation helps the model learn that object identity is independent of orientation which improves recognition accuracy for objects photographed from various angles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482fdfab",
   "metadata": {},
   "source": [
    "### Horizontal Flip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50998ba8",
   "metadata": {},
   "source": [
    "-Horizontal Flip: Mirrors images horizontally.\n",
    "\n",
    "-Purpose: Many CIFAR-10 objects (cars, ships, animals) are valid when flipped. This doubles the effective dataset size which helps the model learn directional invariance and prevents bias toward objects facing a specific direction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a100d930",
   "metadata": {},
   "outputs": [],
   "source": [
    "flipped_image = tf.image.flip_left_right(original_image).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6c9a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axes[0].imshow(original_image.astype('uint8'))\n",
    "axes[0].set_title('Before Flip', fontsize=12)\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(flipped_image.astype('uint8'))\n",
    "axes[1].set_title('After Horizontal Flip', fontsize=12)\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e86e61",
   "metadata": {},
   "source": [
    "The flipped image is a mirror reflection of the original. This augmentation teaches the model that object identity is independent of horizontal direction preventing directional bias in predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfdb401",
   "metadata": {},
   "source": [
    "### Resizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fedac8",
   "metadata": {},
   "source": [
    "-Resizing : Changes image dimensions from 32x32 to different sizes (e.g., 64x64 or 16x16).\n",
    "\n",
    "-Purpose : CIFAR-10's 32x32 images are small. Resizing allows compatibility with pre-trained models that require larger inputs (e.g., 224x224) and shows the trade-off between image detail and processing speed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ede969",
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_large = tf.image.resize(original_image, [64, 64]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b706dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_small = tf.image.resize(original_image, [16, 16]).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765abd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "axes[0].imshow(resized_small.astype('uint8'))\n",
    "axes[0].set_title('Resized to 16x16\\n(Smaller)', fontsize=12)\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(original_image.astype('uint8'))\n",
    "axes[1].set_title('Original 32x32', fontsize=12)\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(resized_large.astype('uint8'))\n",
    "axes[2].set_title('Resized to 64x64\\n(Larger)', fontsize=12)\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e72b480",
   "metadata": {},
   "source": [
    "Resizing shows the trade-off between image quality and computational efficiency. Larger images (64x64) preserve more detail but require more processing power while smaller images (16x16) lose detail but train faster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77f6ed1",
   "metadata": {},
   "source": [
    "### Brightness Adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753a97bc",
   "metadata": {},
   "source": [
    "-Brightness Adjustment: Adjusts image brightness to simulate different lighting conditions.\n",
    "\n",
    "-Purpose: CIFAR-10 images have varying lighting conditions. Brightness adjustment makes the model robust to different illumination levels, simulates real-world lighting variations, and prevents the model from relying on brightness as a classification feature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5004e968",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized = original_image / 255.0\n",
    "brighter = tf.image.adjust_brightness(normalized, delta=0.3)\n",
    "brighter = tf.clip_by_value(brighter, 0, 1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd94f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "darker = tf.image.adjust_brightness(normalized, delta=-0.3)\n",
    "darker = tf.clip_by_value(darker, 0, 1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f564d3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original range:\", normalized.min(), \"to\", normalized.max())\n",
    "print(\"Brighter range:\", brighter.min(), \"to\", brighter.max())\n",
    "print(\"Darker range:\", darker.min(), \"to\", darker.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db09000",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "axes[0].imshow(darker)\n",
    "axes[0].set_title('Darker (-0.3)', fontsize=12)\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(normalized)\n",
    "axes[1].set_title('Original', fontsize=12)\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(brighter)\n",
    "axes[2].set_title('Brighter (+0.3)', fontsize=12)\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd14411",
   "metadata": {},
   "source": [
    "Brightness adjustment simulates different lighting conditions while preserving object features. This ensures the model recognizes objects regardless of illumination improving real-world performance across various lighting scenarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805fa675",
   "metadata": {},
   "source": [
    "### Visual Comparison of All Preprocessing Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bad13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "\n",
    "axes[0, 0].imshow(original_image.astype('uint8'))\n",
    "axes[0, 0].set_title('Original', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow(normalized_image)\n",
    "axes[0, 1].set_title('Normalized', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "axes[0, 2].imshow(rotated_image.astype('uint8'))\n",
    "axes[0, 2].set_title('Rotated', fontsize=12, fontweight='bold')\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "axes[1, 0].imshow(flipped_image.astype('uint8'))\n",
    "axes[1, 0].set_title('Flipped', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "axes[1, 1].imshow(resized_small.astype('uint8'))\n",
    "axes[1, 1].set_title('Resized Small (16x16)', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "axes[1, 2].imshow(resized_large.astype('uint8'))\n",
    "axes[1, 2].set_title('Resized Large (64x64)', fontsize=12, fontweight='bold')\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "axes[2, 0].imshow(darker)\n",
    "axes[2, 0].set_title('Darker', fontsize=12, fontweight='bold')\n",
    "axes[2, 0].axis('off')\n",
    "\n",
    "axes[2, 1].imshow(normalized_image)\n",
    "axes[2, 1].set_title('Original (normalized)', fontsize=12, fontweight='bold')\n",
    "axes[2, 1].axis('off')\n",
    "\n",
    "axes[2, 2].imshow(brighter)\n",
    "axes[2, 2].set_title('Brighter', fontsize=12, fontweight='bold')\n",
    "axes[2, 2].axis('off')\n",
    "\n",
    "plt.suptitle('Complete Comparison: All Preprocessing Variations', \n",
    "             fontsize=16, fontweight='bold', y=0.99)\n",
    "plt.tight_layout()\n",
    "plt.savefig('complete_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe1151d",
   "metadata": {},
   "source": [
    "Side-by-side comparison of all five preprocessing techniques applied to the same CIFAR-10 image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d79065e",
   "metadata": {},
   "source": [
    "### Applying Techniques to Entire Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fc7104",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_normalized = train_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930ec4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomRotation(0.055),\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomBrightness(0.3)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97fe4f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
